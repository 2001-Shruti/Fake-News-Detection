# Fake News Detection using Machine Learning and Deep Learning

![nb.jfif](attachment:nb.jfif)

#### Importing the neccssary Libraries and Loading the Dataset


import numpy as np
import pandas as pd
import matplotlib.pyplot as py
import seaborn as sns
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.multiclass import OneVsRestClassifier

news_df = pd.read_csv('train (1).csv', names=["json ID", "label", "statement", "subject", "speaker", "job title", "state", "party",
                         "barely true", "false", "half true", "mostly true", "pantson fire","context"])

news_df['label'] = news_df['label'].replace({
    'pants-fire': 0,
    'FALSE': 0,
    'barely-true': 0,
    'half-true': 1,
    'mostly-true': 1,
    'TRUE': 1
})

news_df.head()

news_df.shape

sns.countplot(x='label', data=news_df)

#### Data-Preprocessing

news_df.isna().sum()

news_df = news_df.fillna(' ')

news_df.isna().sum()

news_df['content'] = news_df['speaker']+" "+news_df['statement']

news_df

news_df['content']

##### Performing Stemming
When Stemming is applied to the words in the corpus the word gives the base for that particular word. It is like from a tree with branches you are removing the branches till their stem.


ps = PorterStemmer()
def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [ps.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_df['content'] = news_df['content'].apply(stemming)

news_df['content']

X = news_df['content'].values
y = news_df['label'].values

print(X)

##### Applying Tf-Idf Vectorization 
Term frequencyâ€“inverse document frequency, is a technique for text vectorization based on the Bag of words (BoW) model. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.

vector = TfidfVectorizer()
vector.fit(X)
X = vector.transform(X)

print(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y, random_state=42)

X_train.shape

X_test.shape

### Machine Learning Algorithms

#### Logistic Regression (LR)

model = LogisticRegression(solver='saga', max_iter=1000)
model.fit(X_train,y_train)
test_y_pred = model.predict(X_test)
print("test accurracy :",accuracy_score(test_y_pred,y_test))
confusion_mat = confusion_matrix(y_test, test_y_pred)
confusion_mat_display = ConfusionMatrixDisplay(confusion_mat).plot()

#### Support Vector Machine (SVM)

svm_model = OneVsRestClassifier(SVC(kernel='linear'))
svm_model.fit(X_train, y_train)
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)
print('Accuracy:', accuracy)
confusion_mat_display = ConfusionMatrixDisplay(confusion_mat).plot()

#### Naive Bayes Algorithms (NB)

from sklearn.naive_bayes import MultinomialNB
NB = MultinomialNB()
NB.fit(X_train, y_train)
y_pred = NB.predict(X_test)
print('Accuracy of NB  classifier on training set: {:.2f}'.format(NB.score(X_train, y_train)))
confusion_mat = confusion_matrix(y_test, y_pred)
confusion_mat_display = ConfusionMatrixDisplay(confusion_mat).plot()

#### Decision Tree (DT)

clf = DecisionTreeClassifier(max_depth=5)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
confusion_mat = confusion_matrix(y_test, y_pred)
confusion_mat_display = ConfusionMatrixDisplay(confusion_mat).plot()

#### Random Forest (RF)

clf = RandomForestClassifier(n_estimators=100, max_depth=10, criterion='entropy', random_state=42, class_weight='balanced', bootstrap=True)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:',accuracy)
confusion_mat = confusion_matrix(y_test, y_pred)
confusion_mat_display = ConfusionMatrixDisplay(confusion_mat).plot()

#### Passive Aggressive Algorithm

clf = PassiveAggressiveClassifier(max_iter=500)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
confusion_mat = confusion_matrix(y_test, y_pred)
confusion_mat_display = ConfusionMatrixDisplay(confusion_mat).plot()

